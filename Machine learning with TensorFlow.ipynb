{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how do we use all this for machine learning? \n",
    "\n",
    "At this point, you're likely ready to move onto any other TensorFlow tutorial you find on the Internet (or in this Github repo). But in case you want to stay here, we'll work our way through a simple example: we'll create a fake dataset of _temperatures_ and _number of hospital visits_ in a fictional city over a number of months. (We assume that colder temperatures lead to more hospital visits.) We'll then fit a linear model to this data. \n",
    "\n",
    "The goal of this exercise is for you to see how placeholders, variables, and gradient descent come together to enable fitting a model to some data. Although we'll use a _linear model_ here -- i.e., we assume that the data _y_ can be understood as a linear function of the input $x$ -- the process is the exact same when using a nonlinear model, like a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the fake data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by creating some fake data for us to train on. We'll make 1000 samples. We generate temperatures according to a normal distribution using `numpy`, then we generate hospital visit numbers according to the formula, `1000 - 5*temps`, plus some random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x268d619e668>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3X+QH3Wd5/HnO5MvYYIrEyR6MhATlQonskt0SnObuy2BXYKgMIW64LInu0sVdXXerrJU1mS1LrDnLbGyd+DWedxx6oorh0FgxyjeRorEso6qsE6caIiSJaKGTNglFkxOyZDMTD73x7d78p3vt7u/3f3t77e7v/16VKUy3/7299vd3+/M592f9+eXOecQEZHqWZT3CYiISD4UAEREKkoBQESkohQAREQqSgFARKSiFABERCpKAUBEpKIUAEREKkoBQESkohbnfQJRzj33XLdy5cq8T0NEpFT27NnzC+fc8nb7FToArFy5kvHx8bxPQ0SkVMzs53H2UwpIRKSiFABERCpKAUBEpKIUAEREKkoBQESkogrdC0jKY2xikq07DnBkaprzhgbZsH41o2uG8z4tEYmgACAdG5uYZNOj+5iemQNgcmqaTY/uA1AQECkwpYCkY1t3HJgv/H3TM3Ns3XEgpzMSkTgUAKRjR6amE20XkWJQAJCOnTc0mGi7iBSDAoB0bMP61QzWBhZsG6wNsGH96pzOSETiUCOwdMxv6FUvIJFyUQCQTIyuGVaBL1IybVNAZvZFM3vRzJ5u2LbVzJ4xsx+a2d+Z2VDDc5vM7KCZHTCz9Q3br/K2HTSzjdlfioiIJBGnDeBLwFVN2x4H3u6c+3XgH4FNAGb2NuBG4GLvNf/dzAbMbAD4HPBe4G3Ah719RUQkJ20DgHPuu8BLTdu+7Zyb9R7uBs73fr4O+Kpz7oRz7qfAQeBd3r+DzrnnnHMnga96+4qISE6y6AX0R8D/8X4eBp5veO6wty1sewszu9XMxs1s/OjRoxmcnoiIBOkoAJjZJ4FZ4AF/U8BuLmJ760bn7nPOjTjnRpYvb7uimYiIpJS6F5CZ3Qy8D7jCOecX5oeBCxp2Ox844v0ctl1ERHKQqgZgZlcBnwCudc4db3hqO3CjmS0xs1XAhcA/AN8DLjSzVWZ2BvWG4u2dnbqIiHSibQ3AzB4E3gOca2aHgc3Ue/0sAR43M4Ddzrl/55zbb2YPAT+inhr6qHNuznuf/wDsAAaALzrn9nfhekREJCY7nb0pnpGRETc+Pp73aYiIlIqZ7XHOjbTbT3MBiYhUlAKAiEhFKQCIiFSUAoCISEUpAIiIVJQCgIhIRSkAiIhUlAKAiEhFKQCIiFSUAoCISEUpAIiIVJQCgIhIRSkAiIhUlAKAiEhFKQCIiFSUAoCISEWlXhNYoo1NTLJ1xwGOTE1z3tAgG9avZnTNcN6nJSIyTwGgC8YmJtn06D6mZ+YAmJyaZtOj+wAUBESkMJQC6oKtOw7MF/6+6Zk5tu44kNMZiYi0UgDogiNT04m2i4jkQQGgC84bGky0XUQkDwoAXbBh/WoGawMLtg3WBtiwfnVOZyQi0kqNwF3gN/SqF5CIFJkCQJeMrhlWgS8ihaYUkIhIRakGIJWggXkirfoyAJT5j73M515UGpgnEqzvUkD+H/vk1DSO03/sYxOTeZ9aW2U+9yLTwDyRYH0XAMr8x17mcy8yDcwTCdY2AJjZF83sRTN7umHbOWb2uJk96/2/zNtuZvbXZnbQzH5oZu9oeM3N3v7PmtnN3bmccv+xF+HcxyYmWbdlJ6s2Psa6LTv7ovahgXkiweLUAL4EXNW0bSPwhHPuQuAJ7zHAe4ELvX+3AvdCPWAAm4F3A+8CNvtBI2tl/mPP+9z7NQWlgXkiwdoGAOfcd4GXmjZfB9zv/Xw/MNqw/cuubjcwZGZvBNYDjzvnXnLOvQw8TmtQyUSZ/9jzPvd+TUGNrhnmrusvYXhoEAOGhwa56/pL1AAslZe2F9AbnHMvADjnXjCz13vbh4HnG/Y77G0L2565Mo/Czfvcw1JNk1PTjE1MluIzDKOBeSKtsu4GagHbXMT21jcwu5V6+ogVK1akOoky/7Hnee7nDQ0yGRIE1G1SpP+k7QX0z15qB+//F73th4ELGvY7HzgSsb2Fc+4+59yIc25k+fLlKU9P0ghKQfn6IRUkIgulDQDbAb8nz83A1xu2f8TrDbQWOOalinYAV5rZMq/x90pvmxSInysPU4aeVCISX9sUkJk9CLwHONfMDlPvzbMFeMjMbgEOAR/ydv8WcDVwEDgO/CGAc+4lM/tPwPe8/f7COdfcsCzkPxJ4dM0wW3ccCEwFlaEnVRby/g5EesWcC0zFF8LIyIgbHx/P+zR6pnnKAqj3Aup1j5WinEevjU1Mcsf2/UxNzyzYXoVrl/5iZnuccyPt9uu7kcBlVpRumFXsNukHvebCH9T+If2rLyeDK6sijAT2pemNVObUSVDwbaT2D+lHCgAFEtYNM+/ce5yCvewzbrYr4PP+DkS6QSmgAsl7JHCQuNNDxElfFXmeoagCPu/vQKRbVAMokLxHAgeJKtgbz6td+irrGkJzreSyi5az65mjqT+3DetXtzR8AyxbWmPz+y8uRS1GJCkFgIIp2ijmuO0SYemrRWbzhXWcQBJHUDD5yu5D88+nCS5FDL4i3aYA0IEyN3rGFbddIuwOes65wO2+NI2r7RpsIV1wKVrwFek2tQGk1K9TJzeL2y7hdx0dsNZpn6Zn5gK3Q7rG1bhBQz13RKIpAKRUlD773ZZkTMDommFOhQwsnHOO2qLWIHD85GzioBk3aKjnjkg0pYBSKlKf/W5LkhoJSxktW1rjVydmW7a/fHwmcb4+LN3UqNs9d6qQ/pP+pxpASnmv3lVUYSkj52BmLrh2kLTmFFQr+f21K3o2crmf039F7qor2VMNIKWgu1D1Fw/vTXPbtr2Rr0tac8qqwTbNnXyWPZqKpOyD+SQ5BYCUyt5tsJspjKDCOWyGUV8eNae0BV6/pv/6NbBJOAWADvS622BWhXYed3ob1q/m4xG1gDxqTmkLvKJO2dGpfg1sEk5tACWRZd45jx5Mo2uGWba0FvjcsqW1XO4w44xeDsqHR3WNLWIOPe45qV2rehQASiLLQjuvO73N7784sODc/P6Lcyk4owq8qIAb1jUWKFzjcJIbhyLORSXdpRRQSWRZaPcihRGVrmreDnSckkqTHgtqyK8tMo6fnA1MV03PzHHH9v0LjnP3DZfOH2fdlp2Fy6GH3Tjc+Y39sb8f5f/7lwJASWRZaHe7B1O7NobmAiVpwRk0EdwjeyYTB5DmAu/swRqvnJzl5eOti8L4pqZn5heNaT5OEXPoYcd++fjM/HW2+36kfykFVBJZVs+7veJX0nRVkoIzKKXxwO5DqdNjo2uGeXLj5fx0yzWctWRx6FiFMI3HKWIOPe6x+3EUu7SnGkBJZF09j7rT67S3UdI74SS1m6DgElZkJ73zTnun7r+uiGND4oya9qm3T/UoAJRIL6rnWXQRjSrQg4JL3IJzbGIycixB0PGSCDvvuMcpYg496JxeOTEbuPaxevtUj7mQybuKYGRkxI2Pj+d9GpWybsvOwEJweGiQJzdeHvq6xoJ96RkDvHKy9Y5z3VvO4fuHjrUU9H4PmqiCszkwNTMW1gT8901S+LY7RpA0x4k6ftBnkPWgvaDrzPI6JH9mtsc5N9JuP9UAZIE0DZnNBUpQ4Q/w5E9eatk2PTPH7Q/9gFPOtfSqaRS1BsBgbYAPvHO4oxXBIPhu+XibRuEPvDO7KSmCal7jP38pVQN3lCLWVCQfqgGUQC9nnkxTAwh7TRphd6IrNz4W+pp7QoJGFsYmJrlt297QdoZ2NaO4wj7DATPmAv5GB8zmg6YKb2kWtwagXkAF1+uZJ9P0Nsqy8dCvETRfX9iCMgNmXS38RtcMc9PaFaHPJ732sAFvYe8TVPj724sy2EzKSwGg4Ho9bUOaLqJZNx76y0g2FmpRBSF0dxrjT49eEjqNRZJrjwrmYe8TFvgaqQunpKUAUHB5DC5q7Bv/5MbLY42orQ0sLKgGFhnti65wzYXacEgBuWxpLVUtKWnACJvGIkkXz6hgHlbz+vC7L2jZHkRdOCUNBYCCK+LgokBNN+iLgJvWrogsvJYtrQUuE+lrLNSCggzAr16d5c5v7E9US0oTMLIYPBcVzMPe/9OjlyzYnuXayiLqBVRwRRxc1GzrjgPMnFoYAWZOOXY9c5S7rr+EO7+xv6UnjT8JXNBzvsZCbXTNMHds39/Sf33mlAt9fViBm3Ya6LjjMMIa7dsNeAt7/8btYV04i/T7IOXRUQ3AzG4zs/1m9rSZPWhmZ5rZKjN7ysyeNbNtZnaGt+8S7/FB7/mVWVxAv+v2tA1ZaHdnO/Efr+SeGy4NvIapiC6WzYXasYDBS1HC7oq7mVaLql1kMZ1HGX4fpDxS1wDMbBj4E+BtzrlpM3sIuBG4GrjbOfdVM/sfwC3Avd7/Lzvn3mpmNwKfAW7o+AoqIM8JuuJ0QY0zlUPYNUQtIh/3OEODNU7Mnop9V9zN2VCjahd+d9FOu/Rm+fugxe2rrdM2gMXAoJktBpYCLwCXAw97z98PjHo/X+c9xnv+CrMYXRwkN3Fz5Z3c2Ya9dvP7L4697x3XXpzorjjofWqLjKnjJ1m58TFWbnyMS+/8dqqeRO1qF0kb2Lupnxe3l3hS1wCcc5Nm9lfAIWAa+DawB5hyzs16ux0G/N/wYeB577WzZnYMeB3wi7TnIN0VN1feycjSJK9tt2/cwjRoGuhfnphdMIJ5anqGDV/7QaL3BRhaWgtskyhiI22SthDVFPpTJymgZdTv6lcBU8DXgPcG7Oq3Dgbd7bd07jazW4FbAVasCB+AI92XJFfeSVoiyWuzSn80vs+6LTsDJ0ebOeUSLebyqbF9oQ3Sr5yYnV9NLInGgndoaQ3n6m0hWRTCcb/fPNaQlt7opBfQbwM/dc4dBTCzR4HfBIbMbLFXCzgfOOLtfxi4ADjspYzOBlomh3HO3QfcB/WpIDo4P+lQERY/78WdZ1Tjb9yG4bGJSR7YfSj0+anpmVSrnDUWvI3BpXGuoLRzIMX9ftP2mpLi66QN4BCw1syWern8K4AfAbuAD3r73Ax83ft5u/cY7/mdrsgTEUnua8QG5ahv27aXT43tS/1+QYO/zh4MHuUL8YPd1h0HQucL8iUdsRs1AZ7/fg/sPpQ6hx/0/Zr3PnGmqdDgs/LrpA3gKTN7GPg+MAtMUL9zfwz4qpl92tv2Be8lXwD+1swOUr/zv7GTE5fuy3vWyLDFXx7YfYiRN53T0VTPjWmMqK4IcYNd3MIwSaEZZ9/moNN4Z96u9tT4/U5OTS+YUrvx8ylCTbBo+qVNpKOBYM65zcDmps3PAe8K2PdV4EOdHE96r9tdUKP+kMIKQAeJ0w9RaYyosQidplOC9osrrEG5nSNT07Hz9v73GzQbaeM0FRp8dlo/tYloKgjJTbtuiFGF5eTUdKKJ38KCyeTUNItCqgBh8w8FCUqnNEtaaKZNkJ43NJjpuswafLZQrydo7CYFAMlNuz+kDetXR04olyTvHRZMjOCZRtOM0P3AO4fn5+oZMGPdW87pqNBsN/J5YJG1zKXkn3eadZmjthdp/ELe+qlNRAFAchNn0NRNa1e0nVU0zt1XWINn0E32gFniwvpTY/t4YPeh+WAy5xzfP3SMDetXpy4026WL5k45XnPm4sAgk3QSwawa/Ls5LXdRlGaCxhgUACQ3cf6QPj16CXc3zCMUpt3dV1AaIyzDcsq5xA3MD+w+FNogm9ZlFy1vG/ymjs/M35lvWL+arTsOsGrjYxw/ORtaOwiSRZqnKiOL8+4dlyXNBiq5idu42DxoK22PlOZZNcOWeky6yMvtD/0gNJhMTk3zlk3f4sPvvoBPj16S6H0f2TPZtmupf65BYwZqA8bQYC32wLFOG/yrMl4g795xWVIAkNyk+UOKChpJuuaF9ds37xhxRuD6hW7YamW+Oef4ijdILG4QaDcGABYGy6D9Z+YcZy1ZzN7NV8Y6Zqf6KTfeTp4TNGZJAUBylfQPKSxoAIm65kV1MW1+r6ARuP45tCukG/3vpw7FHrXbrtBsbqcI64I66XUJDTtOlv3ZNV6gfKzIg3FHRkbc+Ph43qchJRCWGhoeGpyfhjnu/hBeoDbud8TLdac1WBsIzbOHnZ/PgJ9uuWb+8Vs2fSu0JlJbZLzmzMVMHQ+uwTTXptJ28cz6/SQ9M9vjnBtpt58agSVzefQESZp+2LB+dUsjaW2RRXahbH7fTu9soxqJ23WBPW9ocMHnHJWG8ldNa26Yzbo/u8YLlI9SQJKpvEZJpko/NJewFv1eze8b1B6RVFSwiapdXHbR8tTH9gv5buTs+yU3XhWqAUim8holGdU1L6hGsnXHAWbmmtYxnnPc/tAP2na/9N/Xv+MdCphMbhHgVzAGzBisBf+pBQUoP4iGGRqsseuZox0HnrL1Z6/CGINeUwCQTOXVEyQs/QAE9k0Pu8Ofc45t33s+8u67Ma0xumaYvZuv5PfXrlgwCvj31q7gubuu4WdbruEnd13NXdf/euy+41GNy/4KaJ1+nn4NJkl/9jwL4KqMMeg1pYAkU2lSMVn1RAlKP6zbsjOwRjJgFpo3n5lzLDI4FfD08NBg4GpZj+yZXDAK+JE9ky0zlp5ZWzR/LkODNe649uJEPZTgdPDxZ/BsZ7C2iBOzpxZci9/WkaQbbt4ToFVljEGvKQBIppLOHNntgiWsMJ1zjsHaQOid9ilHy/NJ7tibp2Vu/kxOzJ4KPeewINoYfOK2P5xZG2D2lONUY7qrIb8VN2cfdo23P5R82cw0qjTGoJeUApJMJe0J0u02g7CaR2OKKEyc6xibmAy9E/cLp3bX2Jxaueyi5ZGpmcYePANRixlQH8MQ1NaR9PONCqS9SMWELdoTtZiPtKcagGQuSU+Qbt/ZRdVIRtcMc8f2/YHrAQ8N1tpeR7vGWj/4RF1jUA3okT2TfOCdw+x65iiTU9MMmDE9M8cd2/fzyb/bt2Dxer8mc2ZtUaK1A5J+vlE9o3qRigmLc23in7ShGoDkKk5PlE4aH9vVSO649uKW8QCLqBcs7Y4X1VhbW2QcPznLqo2Pha43EDVv/65njs430vptC1PTMwsK/8b9nZeyiitpT5926x34I467JWzRnqjFfDpVhV5HqgFIrtq1GWTRRhB1J9/cEHr2YI1XTs7O301HHS+yEdZOTyER1NhcGzBeOTEbWPvw3/v2h37Qdp4h37HpGe6+4dL561gU0cidZuZK/9qjzinoc8qqgb/X00zk3ejdK6oBSK7a3aF3s43Av8O7bdteAO6+4VLOWrK4JWcedLyxicnIsQLN7wH17qEGLFtaA0do4e+LW/hDvSBsXLTlVMRr047OHV0zzH/53d8IrQk0f05Zdt3s9RTM/bTqVxTVACR3UXfo3WojCLvDC0vpTE5Ns27Lzvk72eMnZxPPAzTnHD/bcg3rtuxMtdZvmKCCME5PomZx7tb9xx/3gmazxmNm2XWz11MwV6XXkQKAFFq3qv5hhVPY+ADjdOEWp/99EL/HTpaFSNh4gqTdcf0Vzfwrn5ya5rZtexn/+UuBU1iHraZmMD/7aNaFaC+nmajKzKZKAUmhdavq3258QKOwwi6pOedYt2VnaNfFsC6dfupoaLDGsqW1+VTZPTdcyt7NVwYWilGptebGzebC3+eAB3YfaknZhK2l4L/GT5OUbaqJRv206lcU1QCk0LpV9Y9KkfhLK/rHS3vHH2RyapraQH0x95mG4bmDtQE+8M5hHtkz2fF0ys2pnLtvuHTBSmjNqa+gwt/nF+iNx293B+8/n7QWUiT9tOpXFK0HIJWUZO76sLn5hwZr/PLV2USNtb5lS2ssPWNxS+HSXHhfdtHy2IvIQGsqp/m61vzFtxO3PzSvPdBurYLGNRiyXHBG4ou7HoACgFRK2FKPZw/WMKNl0RT/NWHBImxd4XaaC9Wwc02ywErUOsd+zSas8TZK86I6QecV5/ykd7QgjEiT5m6JLx+f4cTsKW5au4ITs6cCF02B6Hx62nx2nNcl7YoYlZv3xxUkFZSyafw84HTbReP0Gv0+gKpfqAYglRGWugjr+RO2nGSjoLvh2iIDCx4LAPHvkldtfCy0p41fe2is0UT9JadpyB5OkbLRspDFELcGoEZgqYyonj9J9m8UtUh94+jisPRSlKiuiGMTk9z5jf2x8/lJCv84gS9MnL7/ahcoDgUAqYywAjWsBhA3vRPWP73TQi2sF02S5SCT3vn74x3WbdnZEsjiFNbt+v5nMcVCUABJep5SpxSQVEZYeiKr7pfdEFTYxVkMxqDt/gNmfPjdF8zPOtocLGoDBo6W7qpRn0tYms3v9RR23nFrHXFTbkX5/vLSkxSQmQ0BnwfeTv1354+AA8A2YCXwM+B3nXMvm5kBnwWuBo4Df+Cc+34nxxdJIqpv98ibzinkHWRQ7aJdT56gwrRdXj6o4A5qw2hshA76vIJqLbUB41evzkamq+KODg5KMc0ELN2m1cLi6TQF9Fng751zHzSzM4ClwJ8DTzjntpjZRmAj8AngvcCF3r93A/d6/4v0TFS6piyFRdRylmG9diA6RZJkeobmeZOC0jiNx4qa9dQXN92W5Dz7bd6ebkgdAMzstcBvAX8A4Jw7CZw0s+uA93i73Q98h3oAuA74sqvnnHab2ZCZvdE590LqsxepoKiBZ2Fpj3YBLumI56iG3uZjrdr4WOR7JRkdnOQ8yzDlRN46GQfwZuAo8DdmNmFmnzezs4A3+IW69//rvf2HgecbXn/Y2yYiCQyHFGxRM322EzT3jT9lRVxhd9xRBXG7JUObXXbR8sDtA03nWZYpJ/LWSQpoMfAO4I+dc0+Z2Wepp3vCBP0mtdzKmNmtwK0AK1as6OD0RPqL3yAc1GDr9w7yp6xO2vW0XXfWOHfdi8xYtfGxluOF9WbyB41t3XGA27btjXWeu545Grj915Ys5qwlrVNrSLTUvYDM7F8Au51zK73H/4Z6AHgr8B7n3Atm9kbgO8651Wb2P72fH/T2P+DvF3YM9QISqQvq/eIHgWFvzqDmnkyN/N5OSeYVahQ2KC1McyPzp8b28eBTzzPn3Hzvo5E3nZN40FicwXHSg6kgnHP/BDxvZn496wrgR8B24GZv283A172ftwMfsbq1wDHl/0XiCer94hf+T268nF3PHI0cFzA9M8dXdh9KvTpXVBonaBrrxt5CYxOTPLJncr7tYs45HtlTH8iWdNWtMk8xXUSdzgX0x8ADZvZD4FLgL4EtwO+Y2bPA73iPAb4FPAccBP4X8O87PLZIKWSxuHi7AVZperwkWeIwbH78e264NHT5Sf+cwkYHh3ULjbqWqszT3ysddQN1zu0FgqoZVwTs64CPdnI8kbLJanHxditUpV23IG7giGojCFuA3j+3pMEp6m4+qkurpphITlNBiHRRVuvitltcJej5OJKkTpq7d/rBLajwbzy3sOA0NFjjxOypxAvGBHVpzSrQVo2mgxbpoqzWxY2akjroeX/5yCidpk6CghvU2wQazy0sbXPHtRdHXlOn55IkxVVVqgGIdFGWi4u3G8wV9Pynxvbxld2HWvZdWlvEX3Y4V05YEDvlXEtKZmhpjSWLF3FsurVbahZ36FkvQF8VqgGIdFHejZZh/eaXnbUkk3WVw7aHLb5z9w2X8uTGyzNPy6h3UDoKACJd1C51023dvDMOCm5GfbRur1MyG9avrs9e2qA2YOod1IZSQCJdludEc3FTUGl60IyuGWb85y8tWITeQeSAtObAk2nPnea26OLOdF8YCgAiOcm622LQ+7XrPeS/Lm0Pml3PHG0pZ6dn5mItshP3uHE+p607DrRMCz1zymlK6DaUAhLJQXOOPOnI3LjvB7RNQXWSrolaZrNdSibOceN+Tnk1AmcxyC9PqgGI5CCr8QFx3q9do2snhWdYimnZ0hq/enV24camCkGc48b9nLLsbRVXP4w9UA1AJAdZ37F2Wogn2d4orJeTc60rdfkpmSTHjXtdefS26oexBwoAIjnIuttip4V42h40Yb2cjoWsANZYcMcptONeVx69rfph7IFSQCI5iNM429P366AHTVAvp7A1BBoL7jhLVSa5rrS9rdI2xueRdsqaAoBIDuIUfr16v270oIlbcMcZ3eyfYzcmeeskj591EM9D6gVhekELwoh0X7cWWSnD7JzrtuwMvIv311lop6jXGHdBGNUARCquW6mMPAfAxdVpHr8M1xhFjcAiFZf3fEV5qvocQgoAIhWX93xFeapy8AOlgESEYqUyeplX73Yjc9EpAIhIYeQxurZIwa/XlAISkcLoh9G1ZaIagEiBFLVbYa/0w+jaMlEAECmIfphcrFPdHF1b9eAaRCkgkYJQ+qN7vXKynn67XygAiBSE0h/d65Kq4BpMKSCRgijb5GLdSql0o1eOgmswBQCRgijT5GJFb69oDk5DS2u8fLx1iuqiBtdeUQpIpCDKNCK3yCmVoHz/r16dbVnzoKjBtZdUAxApkLIMSipySiUoOM2ccgwN1jhryeJEKat+7zmkACAiiRW5vSIsCB2bnmHv5itjv0/R01xZ6DgFZGYDZjZhZt/0Hq8ys6fM7Fkz22ZmZ3jbl3iPD3rPr+z02CKSjyJPopbVDJ9FTnNlJYs2gI8BP254/BngbufchcDLwC3e9luAl51zbwXu9vYTkRIqcntFVsGpyGmurHSUAjKz84FrgP8M/KmZGXA58HveLvcDdwD3Atd5PwM8DPw3MzNX5CXJRCRUUdsrsprhs8hprqx02gZwD/BnwK95j18HTDnnZr3HhwH/Ux8Gngdwzs2a2TFv/190eA4iIgtkEZzK1C03rdQpIDN7H/Cic25P4+aAXV2M5xrf91YzGzez8aNHj6Y9PRGRjhQ5zZWVTmoA64Brzexq4EzgtdRrBENmttirBZwPHPH2PwxcABw2s8XA2cBLzW/qnLsPuA/qi8J3cH4iIh0paporK6lrAM65Tc65851zK4EbgZ3OuZuAXcAHvd1uBr7u/bzde4z3/E7l/0XDHXgNAAAGTUlEQVREFhqbmGTdlp2s2vgY67bs7OqEdd0YCfwJ6g3CB6nn+L/gbf8C8Dpv+58CG7twbBGR0ur1rKWZDARzzn0H+I7383PAuwL2eRX4UBbHExHpR1FjD7qRitJcQCIiBdHrsQcKACIiBZHVKOa4FABERAqi11NsaDI4EZGCyGoUc1wKACIiBdLLsQdKAYmIVJQCgIhIRSkAiIhUlAKAiEhFKQCIiFSUAoCISEUpAIiIVJQCgIhIRSkAiIhUlAKAiEhFKQCIiFSUAoCISEUpAIiIVJQCgIhIRSkAiIhUlAKAiEhFKQCIiFSUAoCISEUpAIiIVJQCgIhIRSkAiIhUlAKAiEhFKQCIiFTU4rxPQEQkjbGJSbbuOMCRqWnOGxpkw/rVjK4Zzvu0SkUBQERKZ2xikk2P7mN6Zg6AyalpNj26D0BBIIHUKSAzu8DMdpnZj81sv5l9zNt+jpk9bmbPev8v87abmf21mR00sx+a2TuyuggRqZatOw7MF/6+6Zk5tu44kNMZlVMnbQCzwO3OuX8JrAU+amZvAzYCTzjnLgSe8B4DvBe40Pt3K3BvB8cWkQo7MjWdaLsESx0AnHMvOOe+7/38S+DHwDBwHXC/t9v9wKj383XAl13dbmDIzN6Y+sxFpLLOGxpMtF2CZdILyMxWAmuAp4A3OOdegHqQAF7v7TYMPN/wssPetub3utXMxs1s/OjRo1mcnoj0mQ3rVzNYG1iwbbA2wIb1q3M6o3LqOACY2WuAR4CPO+f+X9SuAdtcywbn7nPOjTjnRpYvX97p6YlIHxpdM8xd11/C8NAgBgwPDXLX9ZeoATihjnoBmVmNeuH/gHPuUW/zP5vZG51zL3gpnhe97YeBCxpefj5wpJPji0h1ja4ZVoHfoU56ARnwBeDHzrn/2vDUduBm7+ebga83bP+I1xtoLXDMTxWJiEjvdVIDWAf8W2Cfme31tv05sAV4yMxuAQ4BH/Ke+xZwNXAQOA78YQfHFhGRDqUOAM65/0twXh/gioD9HfDRtMcTEZFsaS4gEZGKUgAQEakoq2dmisnMjgI/7/FhzwV+0eNj9oqurZx0beWU57W9yTnXth99oQNAHsxs3Dk3kvd5dIOurZx0beVUhmtTCkhEpKIUAEREKkoBoNV9eZ9AF+nayknXVk6Fvza1AYiIVJRqACIiFaUA0MDMrjKzA96qZRvbv6KYkq7WVkZmNmBmE2b2Te/xKjN7yru2bWZ2Rt7nmIaZDZnZw2b2jPf9/at++d7M7Dbv9/FpM3vQzM4s6/dmZl80sxfN7OmGbaVbDVEBwGNmA8DnqK9c9jbgw94KZ2WUdLW2MvoY9UWIfJ8B7vau7WXgllzOqnOfBf7eOXcR8BvUr7H035uZDQN/Aow4594ODAA3Ut7v7UvAVU3bSrcaogLAae8CDjrnnnPOnQS+Sn0Vs9JJsVpbqZjZ+cA1wOe9xwZcDjzs7VLKazOz1wK/RX2WXZxzJ51zU/TJ90Z97rFBM1sMLAVeoKTfm3Puu8BLTZtLtxqiAsBpsVYsK5uYq7WVzT3AnwGnvMevA6acc7Pe47J+d28GjgJ/46W3Pm9mZ9EH35tzbhL4K+ozBL8AHAP20B/fm6+j1RDzoABwWqwVy8okwWptpWFm7wNedM7tadwcsGsZv7vFwDuAe51za4BXKGG6J4iXD78OWAWcB5xFPTXSrIzfWzuF/f1UADitr1Ysi1qtzXu+cbW2MlkHXGtmP6Oepruceo1gyEstQHm/u8PAYefcU97jh6kHhH743n4b+Klz7qhzbgZ4FPhN+uN784V9T4UtWxQATvsecKHXK+EM6g1U23M+p1RSrNZWGs65Tc65851zK6l/RzudczcBu4APeruV9dr+CXjezPyVza8AfkQffG/UUz9rzWyp9/vpX1vpv7cG5VsN0Tmnf94/6iuW/SPwE+CTeZ9PB9fxr6lXMX8I7PX+XU09V/4E8Kz3/zl5n2uH1/ke4Jvez28G/oH6inNfA5bkfX4pr+lSYNz77saAZf3yvQF3As8ATwN/Cywp6/cGPEi9LWOG+h3+LWHfE/UU0Oe8cmUf9Z5QuV+Dc04jgUVEqkopIBGRilIAEBGpKAUAEZGKUgAQEakoBQARkYpSABARqSgFABGRilIAEBGpqP8PvwPzFCpd2ZoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "temps = np.random.normal(55, 20, 1000)\n",
    "random_noise = np.random.normal(0, 100, 1000)\n",
    "hosp_visits = 1000 - 5 * temps + random_noise\n",
    "plt.plot(temps[:200], hosp_visits[:200], \"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's common to perform some data cleaning operations on our input data before attempting to use a machine learning algorithm. We'll do that here, subtracting the (empirical) mean and dividing by the standard deviation of the temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_temps = (temps - np.mean(temps)) / np.std(temps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's divide the data into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = normalized_temps[:800], hosp_visits[:800]\n",
    "valid_X, valid_y = normalized_temps[800:900], hosp_visits[800:900]\n",
    "test_X, test_y = normalized_temps[900:], hosp_visits[900:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll create a \"batch generator\" for the training set. The following function is a Python _generator function_; instead of returning a value, it continuously _yields_ new batches of data. When we call `batch_generator`, Python creates a _generator iterator_, which we here call `training_generator`, that we can use with Python's `next` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size):\n",
    "    total_batches = len(X) // batch_size\n",
    "    current_batch = 0\n",
    "    while True:\n",
    "        start = batch_size * current_batch\n",
    "        end = start + batch_size\n",
    "        yield (X[start:end], y[start:end])\n",
    "        current_batch = (current_batch + 1) % total_batches\n",
    "\n",
    "training_generator = batch_generator(train_X, train_y, batch_size=100)\n",
    "# Later, call next(training_generator) to get a new batch of the form (X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to build our model. Let's first get a new, empty graph to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "g = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then set up the major quantities in our model as follows:\n",
    "1. Our data, the temperatures `X` and the hospital visit numbers `y`, are represented with placeholders. This is so we can fill these values with our actual data at execution time. (You may wonder: why not just put all the data in as a constant? Typically, rather than use Gradient Descent, we use _Stochastic_ Gradient Descent, which means that instead of taking gradients of the loss computed on _all_ our data _every_ iteration, we feed in a small \"batch\" of data to the graph each iteration of training. This is more efficient, lets us handle large datasets, and provably converges to a local minimum just like normal Gradient Descent. To use this technique, we need placeholders: each time we call `sess.run`, we'll pass in different data.)\n",
    "2. The parameters of our model, which we hope to _learn_, are represented as TensorFlow variables. This is so that as we run the training operation repeatedly, their current values can change.\n",
    "3. We then use TensorFlow operations like addition and multiplication to create predicted `y` values based on the `X` values, according to our model. The loss will be computed based on this prediction and its divergence from the true `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "m = tf.Variable(0.)\n",
    "b = tf.Variable(0.)\n",
    "predicted_y = X * m + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the loss -- a quantity measuring how bad our model is -- we use sum-of-squares formula: for each data point in the current batch of data, we subtract the real `y` from our `predicted_y` and square the difference; then we take the average of all these. (Taking their sum would work just as well, but by taking the average, we get a number that doesn't depend on the amount of data in a batch, which can be useful for human interpretation and for comparing models with different batch sizes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss = tf.reduce_mean(tf.squared_difference(predicted_y, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to actually create our optimization op. We'll use the basic `GradientDescentOptimizer` here, with a learning rate of `0.0005`. (How did we come by this number? Trying different numbers and seeing how the losses looked on the validation set. Feel free to play with this a bit more.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_step = tf.train.GradientDescentOptimizer(learning_rate=0.0005).minimize(avg_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing variables and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it's time for training. Let's add an op to the graph that initializes all variables, then start a session and run the training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_all_vars = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: training loss = 547211.375, validation loss = 518814.875, m=-0.03698606789112091, b=0.7282298803329468\n",
      "Iter 500: training loss = 203055.59375, validation loss = 192593.640625, m=-35.43458557128906, b=285.6358642578125\n",
      "Iter 1000: training loss = 84924.3828125, validation loss = 73819.1015625, m=-58.61866760253906, b=458.62847900390625\n",
      "Iter 1500: training loss = 35738.546875, validation loss = 30952.3359375, m=-73.71553802490234, b=563.725341796875\n",
      "Iter 2000: training loss = 20211.064453125, validation loss = 15739.90234375, m=-83.48970794677734, b=627.5347900390625\n",
      "Iter 2500: training loss = 13287.3583984375, validation loss = 10489.923828125, m=-89.78942108154297, b=666.3272094726562\n",
      "Iter 3000: training loss = 10772.7333984375, validation loss = 8779.498046875, m=-93.83386993408203, b=689.8665161132812\n",
      "Iter 3500: training loss = 10380.283203125, validation loss = 8284.443359375, m=-96.41941833496094, b=704.1984252929688\n",
      "Iter 4000: training loss = 9264.0361328125, validation loss = 8185.576171875, m=-98.06950378417969, b=712.8782348632812\n",
      "Iter 4500: training loss = 10046.521484375, validation loss = 8199.9033203125, m=-99.11656188964844, b=718.183349609375\n",
      "m: -99.78704071044922, b: 721.3650512695312, test loss: 10807.7177734375\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init_all_vars)\n",
    "    for i in range(5000):\n",
    "        X_batch, y_batch = next(training_generator)\n",
    "        feed_dict = {X: X_batch, y: y_batch}\n",
    "        _, loss, m_pred, b_pred = sess.run([train_one_step, avg_loss, m, b], feed_dict=feed_dict)\n",
    "        if i % 500 == 0:\n",
    "            validation_feed_dict = {X: valid_X, y: valid_y}\n",
    "            valid_loss = sess.run(avg_loss, feed_dict=validation_feed_dict)\n",
    "            print(\"Iter {}: training loss = {}, validation loss = {}, m={}, b={}\".format(i, loss, valid_loss, m_pred, b_pred))\n",
    "    \n",
    "    test_feed_dict = {X: test_X, y: test_y}\n",
    "    m_pred, b_pred, loss = sess.run([m, b, avg_loss], test_feed_dict)\n",
    "    print(\"m: {}, b: {}, test loss: {}\".format(m_pred, b_pred, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the standard deviation of the noise in our data is 100, we expect the average loss to be no better than $100^2$, which is exactly what we see here. The recovered slope and y-intercept (-100, 729) may look strange to you, but remember that we cleaned our data, which changed the meaning of these parameters. Since we divided temperature values by the standard deviation, approximately 20, a slope of -100 is actually a slope of -5 per degree. And since we subtracted the mean temperature, 55, a y-intercept of 729 is actually a y-intercept of 1004 (that is, $729 + m * \\text{mean} = 729 + 5 * 55$)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "282px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
